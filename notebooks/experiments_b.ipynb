{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKTBPBJy-MIp"
   },
   "source": [
    "# Q-Learning in TextWorld\n",
    "## Overview\n",
    "*Text Adventure Games* are games in which the player interacts with a rich world only through text. Text adventure games predate computers with graphics. However, in many ways they are more complex than conventional video games because they can involve complicated interactions (e.g., \"build a rope bridge\") that require a fair amount of imagination. Indeed, text adventure games are used as [research testbeds](https://arxiv.org/abs/1909.05398) for natural language processing agents.\n",
    "\n",
    "The canonical text adventure game is [Zork](https://en.wikipedia.org/wiki/Zork), in which the player discover an abandoned underworld realm full of treasure. You can find online playable versions.\n",
    "\n",
    "A text game is made up of individual locations--also called \"rooms\", though they need not be indoor enclosed spaces as the term might imply. The agent can move between rooms and interact with objects by typing in short commands like \"move north\" and \"take lamp\".\n",
    "\n",
    "In this assignment, we will use a special package that implements text worlds for testing agents: [TextWorld-Express](https://github.com/cognitiveailab/TextWorldExpress). Textworld-Express simplifies text worlds in a few ways: it uses a reduced set of text commands, and rooms laid out in a grid.\n",
    "TextWorld-Express also implements a few different game objectives, such as cooking, and searching for coins.\n",
    "TextWorld-Express generates world configurations, so we will need to implement algorithms that are able to complete different game objectives in different world configurations.\n",
    "\n",
    "In this part of the assignment, our agents will play two different games:\n",
    "- **Coin Game**: a game in which the agent must search for and pick up a single coin.\n",
    "- **Map Reader**: a game in which the agent must find a coin and return it to a box at the starting location.\n",
    "\n",
    "**We will be implementing the tabular Q-learning algorithm** (as opposed to neural Q-learning).\n",
    "\n",
    "## Important Notes and Guidelines\n",
    "- You are **only** allowed to use a restricted set of libraries for this assignment. All packages that come with the default Python installation are permitted, as well as any imports we have already provided for you. You may not use any other libraries than the ones we have provided. If you attempt to use other libraries, the autograder will not be able to run your code.\n",
    "- TextWorldExpress requires Java 1.8 or higher to be installed on your system. For more information, see the [TextWorld-Express README](https://github.com/cognitiveailab/TextWorldExpress).\n",
    "- Do not modify any function signatures or the global variables provided in the notebook. You may add additional helper functions as needed - do not add them to separate cells, as they will not be exported in the autograder. **Any helper functions should be nested within the function that uses them.**\n",
    "\n",
    "## Helpful Tips\n",
    "- If you break execution of a cell running the game engine, you may put TextWorld-Express in an un-recoverable state. If this happens, you will need to reset your kernel/runtime.\n",
    "- In the Map Reader game, you cannot use the map information (it isn't helpful anyway).\n",
    "- You cannot (and shouldn't) filter any actions. We've already filtered out the actions that we don't want your agent to have to consider. For example, the \"take map\" action is never helpful, but you must explore it. Your implementations should quickly realize that that action creates a state self-loop and disregard it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMJadvTO-O7z"
   },
   "source": [
    "# Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31674,
     "status": "ok",
     "timestamp": 1728329914705,
     "user": {
      "displayName": "Sean Lao",
      "userId": "06296829002378019923"
     },
     "user_tz": 240
    },
    "id": "Q_jkFnjg-C7N",
    "outputId": "fa55e7fc-2c3d-492c-db1d-443b7a6c9e75"
   },
   "outputs": [],
   "source": [
    "%pip install gymnasium\n",
    "%pip install textworld-express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_ScAIKmf-Vmm"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY THIS CELL\n",
    "\n",
    "from textworld_express import TextWorldExpressEnv\n",
    "import gymnasium\n",
    "from typing import Union\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE \n",
    "# Add any additional imports (from the Python Standard Library only) here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U9ElZek-y5I"
   },
   "source": [
    "# Load a Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for repeatability, initialize the game environment (`ENV` - a global variable that encapsulates the environment). Set the game generator to load a particular game (coin game or map reader game). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "piAle474_SPG"
   },
   "outputs": [],
   "source": [
    "SEED = 3\n",
    "env = TextWorldExpressEnv(envStepLimit=100)\n",
    "\n",
    "# change the game type and parameters here to test different environments in depth\n",
    "# only one of the two options below should be active (i.e. not commented out)\n",
    "\n",
    "### THESE TWO LINES ENABLE THE \"COIN\" GAME\n",
    "game_type = \"coin\"\n",
    "game_params = \"numLocations=5,includeDoors=1,numDistractorItems=0\"\n",
    "\n",
    "### THESE TWO LINES ENABLE THE \"MAPREADER\" GAME\n",
    "# GAME_TYPE=\"mapreader\"\n",
    "# GAME_PARAMS=\"numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0\"\n",
    "\n",
    "\n",
    "env.load(gameName=game_type, gameParams=game_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEUeySWdB_DE"
   },
   "source": [
    "# MDP Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtR7eZ6xvS59"
   },
   "source": [
    "**Observation Parsing Functions**\n",
    "\n",
    "- `parse_inventory()` attempts to pull the inventory line out of an observation.\n",
    "- `obs_location()` attempts to pull the name of the location of the agent out of the observations.\n",
    "- `hash_state()` converts an observation to a hash code a string of unique numbers.\n",
    "- `parse_things()` attempts to pull out all the objects in an observation.\n",
    "- `parse_doors()` attempts to pull out information about all the doors in an observation. It returns a list of tuples containing `\"name_of_door (direction)\"` and whether it is `'open'` or `'closed'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6J9x_gDtzyYQ"
   },
   "outputs": [],
   "source": [
    "def obs_with_inventory(obs: str, inv: str) -> str:\n",
    "    \"\"\"\n",
    "    Add the inventory to the world observation text.\n",
    "    \n",
    "    This function combines the observation text with the inventory\n",
    "    to provide a more complete text observation.\n",
    "    \n",
    "    Args:\n",
    "        obs (str): The observation text.\n",
    "        inv (str): The inventory text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The combined observation and inventory text.\n",
    "    \"\"\"\n",
    "    return obs + '\\n' + inv\n",
    "\n",
    "def parse_inventory(obs: str) -> str:\n",
    "    \"\"\"\n",
    "    Pull the inventory items out of the observation text that includes\n",
    "    the inventory (from obs_with_inventory()).\n",
    "    \n",
    "    This function searches for the inventory in the observation\n",
    "    and returns the items if they are not empty.\n",
    "    \n",
    "    Args:\n",
    "        obs (str): The observation text containing inventory information.\n",
    "    \n",
    "    Returns:\n",
    "        str: The inventory items or 'empty' if there are no items.\n",
    "    \"\"\"\n",
    "    m = re.search(r'Inventory[a-zA-Z0-9 \\(\\)]*:\\s*([a-zA-Z0-9 \\.\\n]+)', obs)\n",
    "    if m is not None:\n",
    "        if 'empty' not in m.group(1):\n",
    "            return m.group(1).replace('\\n', '')\n",
    "    return 'empty'\n",
    "\n",
    "def obs_location(obs: str) -> str:\n",
    "    \"\"\"\n",
    "    Pull the location out of the observation text.\n",
    "    \n",
    "    This function extracts the location from the first sentence\n",
    "    of the observation text.\n",
    "    \n",
    "    Args:\n",
    "        obs (str): The observation text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The location extracted from the observation.\n",
    "    \"\"\"\n",
    "    first_sentence = obs.split('.')[0].split(' ')\n",
    "    start = first_sentence.index('the') + 1\n",
    "    return ' '.join(first_sentence[start:])\n",
    "\n",
    "def hash_state(state: dict) -> str:\n",
    "    \"\"\"\n",
    "    Produces an identifier for a state dictionary.\n",
    "    \n",
    "    This function generates a hash for the given state dictionary,\n",
    "    which is not guaranteed to be unique but should suffice for\n",
    "    identifying the state.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The state dictionary to hash.\n",
    "    \n",
    "    Returns:\n",
    "        str: The string representation of the hash.\n",
    "    \"\"\"\n",
    "    return str(abs(hash(json.dumps(state))))\n",
    "\n",
    "def parse_things(obs: str) -> list[Union[str, list[str]]]:\n",
    "    \"\"\"\n",
    "    Parse the objects out of an observation.\n",
    "    \n",
    "    This function extracts various objects mentioned in the observation\n",
    "    text and returns them as a list.\n",
    "    \n",
    "    Args:\n",
    "        obs (str): The observation text.\n",
    "    \n",
    "    Returns:\n",
    "        list[Union[str, list[str]]]: A list of objects found in the observation.\n",
    "    \"\"\"\n",
    "    things1 = re.findall(r'[yY]ou \\w*\\s*see [aA]? ([a-zA-Z0-9\\- ]+)\\,? that ([a-zA-Z0-9\\-, ]+).', obs)\n",
    "    things2 = re.findall(r'[tT]here is \\w*\\s*([a-zA-Z0-9\\- ]+)\\,? that ([a-zA-Z0-9\\-, ]+).', obs)\n",
    "    things3 = re.findall(r'[tT]here is \\w*\\s*([a-zA-Z0-9\\- ]+)\\.', obs)\n",
    "    things3 = list(filter(lambda s: 'that' not in s, things3))\n",
    "    things4 = re.findall(r'[yY]ou \\w*\\s*see a ([a-zA-Z0-9\\- ]+)\\.', obs)\n",
    "    things4 = list(filter(lambda s: 'door' not in s and 'that' not in s, things4))\n",
    "    return list(map(lambda x: list(x) if type(x) is tuple else x,\n",
    "                    things1 + things2 + things3 + things4))\n",
    "\n",
    "def parse_doors(obs: str, location: str) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse doors out of an observation.\n",
    "    \n",
    "    This function identifies open and closed doors mentioned in the\n",
    "    observation text and returns them along with their status.\n",
    "    \n",
    "    Args:\n",
    "        obs (str): The observation text.\n",
    "        location (str): The current location of the observer.\n",
    "    \n",
    "    Returns:\n",
    "        list[tuple[str, str]]: A list of tuples containing door descriptions and their statuses.\n",
    "    \"\"\"\n",
    "    sentences = obs.split('.')\n",
    "    doors = []\n",
    "    dirs = re.compile('west|east|south|north')\n",
    "    for sentence in sentences:\n",
    "        m_open = re.search(r'open ([a-z\\- ]*door)', sentence)\n",
    "        m_closed = re.search(r'closed ([a-z\\- ]*door)', sentence)\n",
    "        dir = dirs.search(sentence.lower())\n",
    "        if dir is not None:\n",
    "            if m_open is not None:\n",
    "                doors.append((m_open[1] + ' (' + location + ') ' + dir[0], 'open'))\n",
    "            elif m_closed is not None:\n",
    "                doors.append((m_closed[1] + ' (' + location + ') ' + dir[0], 'closed'))\n",
    "    return doors\n",
    "\n",
    "def parse_room(obs: str) -> dict[str, list[Union[str, list[str]]]]:\n",
    "    \"\"\"\n",
    "    Parse room information from an observation.\n",
    "    \n",
    "    This function extracts and sorts the objects found in the room\n",
    "    from the observation text.\n",
    "    \n",
    "    Args:\n",
    "        obs (str): The observation text for the room.\n",
    "    \n",
    "    Returns:\n",
    "        dict[str, list[Union[str, list[str]]]]: A dictionary containing the things in the room.\n",
    "    \"\"\"\n",
    "    things = sorted(parse_things(obs), key=lambda x: x[0] if type(x) is list else x)\n",
    "    return {'things': things}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZWNNrycW83G"
   },
   "source": [
    "**Environment Interaction Functions**\n",
    "\n",
    "These functions allow the agent to interact with the environment in a slightly more friendly way than the default `env.reset()` and `env.step()` functions. They wrap those funtions and do some processing on the data to bundle it in a way that will be easier to work with.\n",
    "\n",
    "`reset_mdp()` takes an environment (e.g., `ENV`) and returns the starting state id and valid actions in the starting state, as a tuple (look in the code below for more details).\n",
    "\n",
    "`do_action_mdp()` takes the name of an action and a pointer to the environment. It returns the state that results from executing the action. It returns 4 values:\n",
    "- state_id: a string that identifies the current state\n",
    "- reward: a floating point number\n",
    "- termination: a boolean indicating whether the episode has ended\n",
    "- infos: a dictionary containing observation, inventory, and valid actions (as above).\n",
    "\n",
    "**Use these functions instead of `env.reset()` and `env.step()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tux5anN-B7ib"
   },
   "outputs": [],
   "source": [
    "def reset_mdp(env: gymnasium.Env, seed: int) -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Reset the environment and the Markov process.\n",
    "\n",
    "    This function resets the environment for new agent runs\n",
    "\n",
    "    Args:\n",
    "        env (Env): The Environment instance\n",
    "\n",
    "    Returns:\n",
    "        state_id str: A string of the current state id\n",
    "        valid_actions list[str]: List of valid actions to take in the state state_id\n",
    "    \"\"\"\n",
    "    _, infos = env.reset(seed=seed, gameFold=\"train\", generateGoldPath=True)\n",
    "    valids: list[str] = infos[\"validActions\"]\n",
    "    if \"inventory\" in valids:\n",
    "      valids.remove(\"inventory\")\n",
    "    if \"look around\" in valids:\n",
    "      valids.remove(\"look around\")\n",
    "    state_id = hash_state(\n",
    "        obs_with_inventory(infos[\"look\"], parse_inventory(infos[\"inventory\"]))\n",
    "    )\n",
    "    return state_id, valids\n",
    "\n",
    "def do_action_mdp(\n",
    "    action: str, env: gymnasium.Env\n",
    ") -> tuple[str, float, bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Take a step in the environment.\n",
    "\n",
    "    Args:\n",
    "        action (str): the choosen action to take in the \"env\"\n",
    "        env (Env): The Environment instance\n",
    "\n",
    "    Returns:\n",
    "        state_id str: A string of the current state id\n",
    "        reward float: A state's reward\n",
    "        termination boolean: Whether the episode has terminated\n",
    "        valid_actions list[str]: List of valid actions to take in the state state_id\n",
    "    \"\"\"\n",
    "    _, reward, done, infos = env.step(action)\n",
    "    valid_actions = infos[\"validActions\"]\n",
    "    if \"inventory\" in valid_actions:\n",
    "      valid_actions.remove(\"inventory\")\n",
    "    if \"look around\" in valid_actions:\n",
    "      valid_actions.remove(\"look around\")\n",
    "    state_id = hash_state(\n",
    "        obs_with_inventory(infos[\"look\"], parse_inventory(infos[\"inventory\"]))\n",
    "    )\n",
    "    return state_id, reward, done, valid_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVKlX5m4CKxc"
   },
   "source": [
    "# Implement Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8gOs8DSrTa4"
   },
   "source": [
    "**Step 1.** Implement the `q_learning()` function. This function takes the following parameters:\n",
    "- env: a pointer to the environment (`ENV`).\n",
    "- num_episodes: the number of episodes to run before termination of the entire algorithm.\n",
    "- threshold: the number of steps in an episode before terminating a single episode.\n",
    "- learning_rate: a number between 0 and 1 controlling how fast the policy is allowed to change.\n",
    "- gamma: the Bellman equation horizon parameter (between 0 and 1).\n",
    "- epsilon: (optional) if epsilon greedy is implemented, this number (0..1) determines the ratio of random to policy-guided actions. A value of 1.0 indicates purely random, and a value of 0.0 indicates purely on-policy.\n",
    "\n",
    "The `q_learning()` algorithm should return a single value: the policy. The policy will be a dictionary-of-dictionaries where the outermost dictionary has a key for each state visited. Each state points to a separate inner dictionary where the keys are actions and the values are q-values. For example:\n",
    "```py\n",
    "{\n",
    "    state1: {\n",
    "        'move north': 0.1,\n",
    "        'move south': 0.0,\n",
    "        'move east': 0.8,\n",
    "        'move west': 0.4\n",
    "    },\n",
    "    state2: {\n",
    "        'move north': 0.01,\n",
    "        'take coin': 1.0,\n",
    "        'move east': 0.05,\n",
    "        'move west': 0.2\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "You will interact with the environment through `reset_mdp()` and `do_action_mdp()`. Please use the state ids we return in both of these functions to uniquely identify a particular state.\n",
    "\n",
    "We recommend you track your algorithm's performance by tracking the total reward of each episode, and the number of step in each episode (fewer is better). If you use purely random action selection, you will see a lot of variance in your total episode reward. If you implement epsilon-greedy, you will see a trend toward more consistent achievement of maximum reward as episode number increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oVpU7tLJCXnu"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY THIS CELL\n",
    "\n",
    "def q_learning(\n",
    "    env: gymnasium.Env,\n",
    "    num_episodes: int,\n",
    "    max_episode_length: int,\n",
    "    learning_rate: float,\n",
    "    gamma: float,\n",
    "    seed: int,\n",
    "    epsilon: float = 1.0,\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Build a Q-Learning policy\n",
    "\n",
    "    Args:\n",
    "        env (Env): The Environment instance\n",
    "        num_episodes (int): The number of episodes to build the table from\n",
    "        max_episode_length (int): The maximum length of an episode to prevent infinite loops\n",
    "        learning_rate (float): A hyperparameter denoting how quickly the agent \"learns\" reward values\n",
    "        gamma (float): The discount rate\n",
    "        epsilon (float): The probability with which you should select a random\n",
    "        action instead of following a greedy policy\n",
    "\n",
    "    Returns:\n",
    "        dict[str, dict[str, float]]: A dictionary of dictionaries mapping a state and action\n",
    "        to a specific reward value. This is what you will build in this algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up q-table\n",
    "    q_table: dict[str, dict[str, float]] = {}\n",
    "    random.seed(seed)\n",
    "\n",
    "    ### YOUR CODE BELOW HERE\n",
    "\n",
    "    raise NotImplementedError # remove this line when you implement the function\n",
    "\n",
    "    ### YOUR CODE ABOVE HERE\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js7x9FBAr_0e"
   },
   "source": [
    "**Step 2.** Set the parameters for your q-learning algorithm. You can change these values. We've provided some completely random values to get you started. \n",
    "\n",
    "**Remember to change the set_parameters() function below to the correct values or your code will not work in the autograder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1paMMHQnCV6L"
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 50\n",
    "MAX_EPISODE_LENGTH = 10\n",
    "LEARNING_RATE = 0.1\n",
    "GAMMA = 0.9\n",
    "EPSILON = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih1dGqD2Yqyk"
   },
   "source": [
    "Test your q-learning implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YCdoOwDaGjjf"
   },
   "outputs": [],
   "source": [
    "q_table = q_learning(\n",
    "    env,\n",
    "    num_episodes=NUM_EPISODES,\n",
    "    max_episode_length=MAX_EPISODE_LENGTH,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=GAMMA,\n",
    "    seed=SEED,\n",
    "    epsilon=EPSILON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1727726613607,
     "user": {
      "displayName": "Chang Che",
      "userId": "11673380918983542921"
     },
     "user_tz": 240
    },
    "id": "Ak7lMeqSFPYL",
    "outputId": "21dd3b8f-149a-45ce-b907-ee90a9382bc8"
   },
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z2k4EkrGaMm"
   },
   "source": [
    "# Implement Code to Run a Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j5OrjrTtxWM"
   },
   "source": [
    "**Step 3.** Implement code to run the policy. This function takes the following parameters:\n",
    "- q_table: your q-table, as specified in step 1.\n",
    "- env: pointer to the environment (e.g., `ENV`).\n",
    "- threshold: the maximum number of steps to take before terminating.\n",
    "\n",
    "Your function should run a single episode from the initial state and return:\n",
    "- A list of actions taken during the episode (e.g., `[act_1, act_2, ... act_n]`).\n",
    "- The total sum reward of all actions taken as a float.\n",
    "\n",
    "Your function will interact with the environment through `reset_mdp()` and `do_action_mdp()`. Be sure to reset the environment before running, and terminate the episode if `do_action_mdp()` indicates the termination boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7jJCxMBTGdv9"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY THIS CELL\n",
    "\n",
    "def run_policy(\n",
    "    q_table: dict[str, dict[str, float]],\n",
    "    env: gymnasium.Env,\n",
    "    seed: int,\n",
    "    max_policy_length: int = 25,\n",
    ") -> tuple[list[str], float]:\n",
    "    \"\"\"\n",
    "    Run a policy from a built Q-Table\n",
    "\n",
    "    Args:\n",
    "        q_table (dict[str, dict[str, float]]): The built Q-Table dictionary\n",
    "        env (gymnasium.Env): The environment in which to run the policy\n",
    "        seed (int): The seed to use\n",
    "        max_policy_length (int): The maximum length of the policy to run\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: The sequence of actions that the policy performed\n",
    "        float: The sum total reward gained from the environment\n",
    "    \"\"\"\n",
    "    actions = []  # Store the entire sequence of actions here\n",
    "    total_reward = 0.0  # Store the total sum reward of all actions executed here\n",
    "\n",
    "    ### YOUR CODE BELOW HERE\n",
    "\n",
    "    raise NotImplementedError # remove this line when you implement the function\n",
    "\n",
    "    ### YOUR CODE ABOVE HERE\n",
    "\n",
    "    return actions, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goBOBYvBYzeR"
   },
   "source": [
    "**Step 4.** Set the threshold value for episode length during policy execution (test time threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KpK_2wuMuZ7d"
   },
   "outputs": [],
   "source": [
    "MAX_POLICY_LENGTH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWOGP5aWY59n"
   },
   "source": [
    "**Step 5.** Run the policy. You should aim to have a total reward of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727726613607,
     "user": {
      "displayName": "Chang Che",
      "userId": "11673380918983542921"
     },
     "user_tz": 240
    },
    "id": "vhrGyDGBGrMP",
    "outputId": "5070b925-d2b9-470b-e022-d130aae20391"
   },
   "outputs": [],
   "source": [
    "plan, total_reward = run_policy(q_table, env, SEED, max_policy_length = MAX_POLICY_LENGTH,)\n",
    "print(\"plan:\", plan)\n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cBfmUDG1XHk"
   },
   "source": [
    "# New Environment: Stochastic Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLeYTiUVY9Tj"
   },
   "source": [
    "The following creates a new type of environment called `StochasticTextWorldExpressEnv`. This environment is the same as the previous environment type, except that some percentage of the time, the action that the agent chooses is not executed and a randomly chosen action is executed instead.\n",
    "\n",
    "When the environment is created the percentage of action randomness (between 0 and 1) is set, where 0.0 means no randomness, and 1.0 means that actions are executed purely randomly.\n",
    "\n",
    "Otherwise, this environment works the same as previously.\n",
    "\n",
    "Passing `debug = True` to the `step()` function will print the action the environment will *really* execute!\n",
    "\n",
    "**NOTE:** The agent is never able to know whether the action it chose was executed or if a different action was executed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GsQWZj0H1WeS"
   },
   "outputs": [],
   "source": [
    "class StochasticTextWorldExpressEnv(TextWorldExpressEnv):\n",
    "    def __init__(self, serverPath=None, envStepLimit=100, stochasticity=0.0):\n",
    "        # Call the super constructor\n",
    "        super().__init__(serverPath, envStepLimit)\n",
    "        # Store the valid actions and stochasticity\n",
    "        self.valid_actions = []\n",
    "        self.stochasticity = stochasticity\n",
    "        self.never_pick = set([\"look around\", \"inventory\"])\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        seed=None,\n",
    "        gameFold=None,\n",
    "        gameName=None,\n",
    "        gameParams=None,\n",
    "        generateGoldPath=False,\n",
    "    ):\n",
    "        # Call the super method\n",
    "        observation, infos = super().reset(\n",
    "            seed, gameFold, gameName, gameParams, generateGoldPath\n",
    "        )\n",
    "        # Update the valid actions\n",
    "        self.valid_actions = infos[\"validActions\"]\n",
    "        return observation, infos\n",
    "\n",
    "    def step(self, action: str, debug=False):\n",
    "        # If a random value is less than the stochasticity target, choose a random action\n",
    "        if random.random() < self.stochasticity:\n",
    "            temp_valids = copy.deepcopy(self.valid_actions)\n",
    "            # Remove inventory and look around from valid actions to choose from\n",
    "            temp_valids = list(set(self.valid_actions).difference(self.never_pick))\n",
    "            # Pick a random action from whatever remains\n",
    "            action = random.choice(temp_valids)\n",
    "        # If debugging flag is on, print the real action that will be executed\n",
    "        if debug:\n",
    "            print(\"[[action]]:\", action)\n",
    "        # Call the super class with either the action passed in or the randomly chosen one\n",
    "        observation, reward, isCompleted, infos = super().step(action)\n",
    "        # Update the valid actions\n",
    "        self.valid_actions = infos[\"validActions\"]\n",
    "        return observation, reward, isCompleted, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKV4lUC1agso"
   },
   "source": [
    "Create the new environment type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OFWWPDS35bMn"
   },
   "outputs": [],
   "source": [
    "gymnasium.register(id='TextWorldExpress-StochasticTextWorldExpressEnv-v0',\n",
    "                   entry_point='__main__:StochasticTextWorldExpressEnv')\n",
    "SENV = StochasticTextWorldExpressEnv(envStepLimit=100, stochasticity=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbB48O0Eang4"
   },
   "source": [
    "Create a game with this environment type and reset the environment (same as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOkAnt267ETX"
   },
   "outputs": [],
   "source": [
    "game_type = \"coin\"\n",
    "game_params = \"numLocations=5,includeDoors=1,numDistractorItems=0\"\n",
    "SENV.load(gameName=game_type, gameParams=game_params)\n",
    "SENV.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yktsPacua4nu"
   },
   "source": [
    "Train in the stochastic Text World environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Y2MoSrPaESpM"
   },
   "outputs": [],
   "source": [
    "q_table = q_learning(\n",
    "    SENV,\n",
    "    num_episodes=NUM_EPISODES,\n",
    "    max_episode_length=MAX_EPISODE_LENGTH,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=GAMMA,\n",
    "    seed = SEED,\n",
    "    epsilon=EPSILON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEGhWdYma8D-"
   },
   "source": [
    "Test the policy. You should aim to have a total reward of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1727726625011,
     "user": {
      "displayName": "Chang Che",
      "userId": "11673380918983542921"
     },
     "user_tz": 240
    },
    "id": "juNfb4q8EXAa",
    "outputId": "4af89615-d803-44ad-ff33-78e4cb604c80"
   },
   "outputs": [],
   "source": [
    "plan, total_reward = run_policy(q_table, SENV, SEED, max_policy_length = MAX_POLICY_LENGTH)\n",
    "print(\"plan:\", plan)\n",
    "print(\"total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4sT-9IcM7Xs"
   },
   "source": [
    "# New Environment: Negative Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXe7_ZhkQq9I"
   },
   "source": [
    "The following creates a new type of environment called `PunishmentTextWorldExpressEnv`. This environment is the same as the previous environment type, except that the agent receives negative reward when it performs actions that are illegal or do not change the world state. For example, trying to close a door that is already closed, or move in a direction that is illegal.\n",
    "\n",
    "Otherwise, this environment works the same as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NW6uZ2Z8M-b9"
   },
   "outputs": [],
   "source": [
    "class PunishmentTextWorldExpressEnv(TextWorldExpressEnv):\n",
    "    def __init__(self, serverPath=None, envStepLimit=100, punishment=0.0):\n",
    "        # Call the super constructor\n",
    "        super().__init__(serverPath, envStepLimit)\n",
    "        # Store the punishment\n",
    "        self.punishment = punishment\n",
    "        # Store the previous observation\n",
    "        self.previous_observation = None\n",
    "\n",
    "    def step(self, action: str):\n",
    "        # Call the super method\n",
    "        observation, reward, isCompleted, infos = super().step(action)\n",
    "        # If the current look is the same as the previous look, then we have performed an illegal action\n",
    "        if infos[\"look\"] == self.previous_observation:\n",
    "            reward = self.punishment\n",
    "        # Store the previous observation\n",
    "        self.previous_observation = infos[\"look\"]\n",
    "        return observation, reward, isCompleted, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKMjQ_cF5_1C"
   },
   "source": [
    "Register and create the new environment type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IIyZMwA3Ox4H"
   },
   "outputs": [],
   "source": [
    "gymnasium.register(id='TextWorldExpress-PunishmentTextWorldExpressEnv-v0',\n",
    "                   entry_point='__main__:PunishmentTextWorldExpressEnv')\n",
    "PENV = PunishmentTextWorldExpressEnv(envStepLimit=100, punishment=-1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2uUZ9496Rm3"
   },
   "source": [
    "Create a game with this environment type and reset the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZjGRhjzCPZGQ"
   },
   "outputs": [],
   "source": [
    "game_type = \"coin\"\n",
    "game_params = \"numLocations=5,includeDoors=1,numDistractorItems=0\"\n",
    "PENV.load(gameName=game_type, gameParams=game_params)\n",
    "obs, infos = PENV.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knWwRNQV6IyA"
   },
   "source": [
    "Train in the punishment Text World environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0K9GoERC6cmS"
   },
   "outputs": [],
   "source": [
    "q_table = q_learning(\n",
    "    PENV,\n",
    "    num_episodes=NUM_EPISODES,\n",
    "    max_episode_length=MAX_EPISODE_LENGTH,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=GAMMA,\n",
    "    seed=SEED,\n",
    "    epsilon=EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDLjkAz66DNm"
   },
   "source": [
    "Test the policy. You should aim to have a total reward of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1727726630148,
     "user": {
      "displayName": "Chang Che",
      "userId": "11673380918983542921"
     },
     "user_tz": 240
    },
    "id": "c1oehdR96fsu",
    "outputId": "8970a809-4059-4360-91dc-a7f66ebb3819"
   },
   "outputs": [],
   "source": [
    "plan, total_reward = run_policy(q_table, SENV, SEED, max_policy_length = MAX_POLICY_LENGTH)\n",
    "print(\"plan:\", plan)\n",
    "print(\"total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywMkXNMwQsgZ"
   },
   "source": [
    "# Testing Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR9ZHmgw8UFa"
   },
   "source": [
    "This function will run  your agent on an environment, game type, game parameters/configurations, and a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_environment(env: gymnasium.Env, game_type: str, game_params: str, seed: int, parameters: dict[str, int | float]):\n",
    "  print(f\"TESTING {type(env)}, {game_type}, {game_params}, {seed}\")\n",
    "  # Run the q learner and get the policy\n",
    "\n",
    "  # load the environment\n",
    "  env.load(gameName=game_type, gameParams=game_params)\n",
    "  env.reset(seed=seed, gameFold=\"train\", generateGoldPath=True)\n",
    "  \n",
    "  q_table = q_learning(\n",
    "      env,\n",
    "      num_episodes=parameters[\"NUM_EPISODES\"],\n",
    "      max_episode_length=parameters[\"MAX_EPISODE_LENGTH\"],\n",
    "      learning_rate=parameters[\"LEARNING_RATE\"],\n",
    "      gamma=parameters[\"GAMMA\"],\n",
    "      epsilon=parameters[\"EPSILON\"],\n",
    "      seed=seed\n",
    "  )\n",
    "  # run the policy to get the plan\n",
    "  plan, total_reward = run_policy(\n",
    "      q_table, env, seed, max_policy_length=parameters[\"MAX_POLICY_LENGTH\"]\n",
    "  )\n",
    "  # Store the plan in the results\n",
    "  return plan, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will run your agent on all provided environment configurations and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(environments: list[gymnasium.Env], games: dict[str, list[str]], seeds: list[int], parameters: dict[str, int | float]) -> tuple[dict, dict]:\n",
    "    # Results will contain a key (env type, game type, game params, seed) and values will be plans and total_rewards\n",
    "    plans: dict = {}\n",
    "    rewards: dict = {}\n",
    "    # Iterate through all environments given\n",
    "    for env in environments:\n",
    "        # Iterate through all game types, the keys of the games dict\n",
    "        for game_type in games:\n",
    "            # Iterate through all game parameters for the given game type in game dict\n",
    "            for params in games[game_type]:\n",
    "                # Iterate through all seeds\n",
    "                for seed in seeds:\n",
    "                    # Store the plan in the results\n",
    "                    plan, reward = run_environment(env, game_type, params, seed, parameters)\n",
    "                    plans[type(env), game_type, params, seed] = plan\n",
    "                    rewards[type(env), game_type, params, seed] = reward\n",
    "    return plans, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaZoBv5v8Qa9"
   },
   "source": [
    "Set parameters. These are the **final parameters that will be used in the autograder to benchmark your solution**. We have filled in some random values to get you started - you should change these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-DaxveUb8PQv"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY THIS CELL\n",
    "\n",
    "def set_parameters() -> dict[str, int | float]:\n",
    "    parameters = {}\n",
    "\n",
    "    ## FILL IN PARAMETERS BELOW\n",
    "    NUM_EPISODES = 100\n",
    "    MAX_EPISODE_LENGTH = 15\n",
    "    LEARNING_RATE = 0.1\n",
    "    GAMMA = 0.35\n",
    "    EPSILON = 0.80\n",
    "    MAX_POLICY_LENGTH = 20\n",
    "    ## FILL IN PARAMETERS ABOVE\n",
    "    \n",
    "    \n",
    "    parameters[\"NUM_EPISODES\"] = NUM_EPISODES\n",
    "    parameters[\"MAX_EPISODE_LENGTH\"] = MAX_EPISODE_LENGTH\n",
    "    parameters[\"LEARNING_RATE\"] = LEARNING_RATE\n",
    "    parameters[\"GAMMA\"] = GAMMA\n",
    "    parameters[\"EPSILON\"] = EPSILON\n",
    "    parameters[\"MAX_POLICY_LENGTH\"] = MAX_POLICY_LENGTH\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzB7i03p8dij"
   },
   "source": [
    "Run the cell below to execute all tests we have given you. You should aim for a total reward of 1.0 on each environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116437,
     "status": "ok",
     "timestamp": 1727726747581,
     "user": {
      "displayName": "Chang Che",
      "userId": "11673380918983542921"
     },
     "user_tz": 240
    },
    "id": "Xxm2giWM8fcJ",
    "outputId": "63542409-e7e9-4a8e-cf36-49a89ba2f867"
   },
   "outputs": [],
   "source": [
    "## CHANGE THIS CELL AT YOUR OWN PERIL - WE HAVE TESTED IT WORKS AS IS\n",
    "seeds = list(range(5))\n",
    "environments = [\n",
    "    TextWorldExpressEnv(envStepLimit=100),\n",
    "    StochasticTextWorldExpressEnv(envStepLimit=100, stochasticity=0.25),\n",
    "    PunishmentTextWorldExpressEnv(envStepLimit=100, punishment=-1.0),\n",
    "]\n",
    "games = {\n",
    "    \"coin\": [\n",
    "        \"numLocations=5,includeDoors=1,numDistractorItems=0\",\n",
    "        \"numLocations=6,includeDoors=1,numDistractorItems=0\",\n",
    "        \"numLocations=7,includeDoors=1,numDistractorItems=0\",\n",
    "        \"numLocations=10,includeDoors=1,numDistractorItems=0\",\n",
    "    ],\n",
    "    \"mapreader\": [\n",
    "        \"numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0\",\n",
    "        \"numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0\",\n",
    "        \"numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0\",\n",
    "        \"numLocations=15,maxDistanceApart=8,includeDoors=0,maxDistractorItemsPerLocation=0\",\n",
    "    ],\n",
    "}\n",
    "parameters = set_parameters()\n",
    "\n",
    "plans, rewards = run_all(environments, games, seeds, parameters)\n",
    "\n",
    "print(\"All Plans\")\n",
    "print(plans)\n",
    "\n",
    "print(\"Rewards for each configuration\")\n",
    "print(rewards)\n",
    "\n",
    "print(\"Total Reward\")\n",
    "print(sum(list(rewards.values())))\n",
    "\n",
    "print(\"Max Reward\")\n",
    "print(len(list(rewards.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below with a specific environment to see how your agent performs. Feel free to experiment with different environments and seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the game configuration below (any of the config options in the cell above should work)\n",
    "environment = TextWorldExpressEnv(envStepLimit=100)\n",
    "game_type = \"coin\"\n",
    "game_params = \"numLocations=7,includeDoors=1,numDistractorItems=0\"\n",
    "seed = 4\n",
    "## Change the game configuration above\n",
    "\n",
    "\n",
    "parameters = set_parameters()\n",
    "\n",
    "plan, reward = run_environment(environment, game_type, game_params, seed, parameters)\n",
    "print(\"Environment: \", type(environment))\n",
    "print(\"Game Type: \", game_type)\n",
    "print(\"Game Parameters: \", game_params)\n",
    "print(\"Plan: \", plan)\n",
    "print(\"Reward: \", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acQaep88V13E"
   },
   "source": [
    "# Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDcg7IENXtj1"
   },
   "source": [
    "Grading will consist of testing all environments (regular, stochastic, punishment), all games (coin, mapreader), with a variety of parameters per game, and different seeds. There will be 240 tests in total. The code we have provided you will run 30 of these tests. The rest are hidden tests, which run the same code but with different seeds and configurations. You have the ability to load ANY configuration from the TextWorld environment, so you are highly encouraged to test your code with a variety of configurations to ensure it succeeds in the hidden tests.\n",
    "\n",
    "**Grading:**\n",
    "\n",
    "The maximum grade for this portion of the assignment is **60 points.** Each test (run on one environment configuration that is loaded) is worth 0.25 points. The autograder will run all 30 tests that we have provided you in this notebook, and 210 more tests. We have provided you with all the code you need to test any possible configuration, so you should be able to test your code with a variety of configurations to ensure it works well on the hidden tests.\n",
    "\n",
    "## Important Details\n",
    "Grading will be conducted by visual inspection of the code and autograder results. The autograder will display \"sanity check\" results to help you verify that your code behaves the same in the autograder as it does locally. These tests are a subset of the full autograder, and will test the some of the same configurations that we have provided. It is your responsibility to test your code and verify its correctness, and you should use the provided resources to do so. \n",
    "\n",
    "We will also inspect the entire notebook to check if your algorithm implementations include details that are inconsistent with the assignment (e.g., hard-coding values or actions to pass tests) and to make sure no cells were altered to provide unearned grading results. Doing so will result in a grade of 0 for the entire assignment, and may be reported to the Office of Student Integrity.\n",
    "\n",
    "Your submissions are also subject to plagiarism checks - as a reminder, all code must be written by yourself, and no one else (classmates, excessive internet resources, LLMs, etc.). You are permitted to use course resources to help you complete the assignment. Any violations of this will receive a 0 and may be reported to the Office of Student Integrity for further investigation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA0XSuitsCP5"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Upload this notebook with the name `submission.ipynb` file to Gradescope. The autograder will **only** run successfully if your file is named this way. You must ensure that you have removed all print statements from **your** code, or the autograder may fail to run. Excessive print statements will also result in muddled test case outputs, which makes it more difficult to interpret your score. \n",
    "\n",
    "We've added appropriate comments to the top of certain cells for the autograder to export (`# export`). You do NOT have to do anything (e.g. remove print statements) to cells we have provided - anything related to those have been handled for you. You are responsible for ensuring your own code has no syntax errors or unnecessary print statements. You ***CANNOT*** modify the export comments at the top of the cells, or the autograder will fail to run on your submission.\n",
    "\n",
    "You should ***not*** add any cells that your code requires to the notebook when submitting. You're welcome to add any code as you need to extra cells when testing, but they will not be graded. Only the provided cells will be graded. As mentioned in the top of the notebook, **any helper functions that you add should be nested within the function that uses them.**\n",
    "\n",
    "If you encounter any issues with the autograder, please feel free to make a post on Ed Discussion. We highly recommend making a public post to clarify any questions, as it's likely that other students have the same questions as you! If you have a question that needs to be private, please make a private post."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "f0GaE8xE-T6A"
   ],
   "provenance": [
    {
     "file_id": "1b6CKNiIg3Vjt-WA-yOsNgFDydDpy05SG",
     "timestamp": 1726432703432
    }
   ]
  },
  "kernelspec": {
   "display_name": "a4-cs3600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
